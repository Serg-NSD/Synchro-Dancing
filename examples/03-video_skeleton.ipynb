{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the task\n",
    "\n",
    "There is a video file `dance.mp4` with three girls performing shuffle-dance.   \n",
    "It is necessary to pose each girl in each frame, save the result in the video file `dance_out.mp4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, cv2\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration of required functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering function with the conversion of an array into a vector. \n",
    "def center(Xar, F=True):\n",
    "    newX = Xar - np.mean(Xar, axis = 0)\n",
    "    newX = newX.flatten() if F else newX\n",
    "    return newX\n",
    "\n",
    "def affine_transform(ref_keys, tst_keys, ref_confs, tst_confs):\n",
    "    ref_keys = ref_keys\n",
    "    tst_keys = tst_keys\n",
    "    # pad and unpad add and remove 1 at the end of the matrix.  \n",
    "    pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])\n",
    "    unpad = lambda x: x[:, :-1]\n",
    "    X = pad(tst_keys)\n",
    "    Y = pad(ref_keys)\n",
    "    A, res, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n",
    "\n",
    "    # Converting too small values to \"0\".\n",
    "    A[np.abs(A) < 1e-10] = 0\n",
    "\n",
    "    # Now that we have found the extended matrix A,\n",
    "    # we can transform the input set of key points.\n",
    "    transform = lambda x: unpad(np.dot(pad(x), A))\n",
    "    keypoints_transformed = transform(tst_keys)\n",
    "    return keypoints_transformed\n",
    "\n",
    "# The cosine similarity calculation function.  \n",
    "def cosine_similarity(pose1, pose2):\n",
    "    pose1, pose2 = [center(i) for i in [pose1, pose2]]\n",
    "    return np.dot(pose1, pose2.T) / (np.linalg.norm(pose1)*np.linalg.norm(pose2))\n",
    "\n",
    "# Weighted cosine similarity calculation function.  \n",
    "def weighted_distance(pose1, pose2, confs):\n",
    "    # Centering.  \n",
    "    pose1, pose2 = [center(i, False) for i in [pose1, pose2]]\n",
    "\n",
    "    # Normalization\n",
    "    pose1, pose2 = [i/np.linalg.norm(i) for i in [pose1, pose2]]\n",
    "\n",
    "    # Summation of weighted distances between keypoints.  \n",
    "    sum = 0\n",
    "    for k in range(len(pose1)):\n",
    "        sum += (confs[k] \n",
    "        * np.linalg.norm(pose1[k]-pose2[k]))\n",
    "    return sum / confs.sum()\n",
    "\n",
    "def draw_skeleton_per_person(\n",
    "            img, all_keypoints, all_scores, confs, \n",
    "            all_boxes,\n",
    "            keypoint_threshold=2, conf_threshold=0.9,\n",
    "            thickness=2, rd=5, th=2):    \n",
    "\n",
    "    cmap = plt.get_cmap('rainbow')\n",
    "    img_copy = img.copy()\n",
    "    color_id = (np.linspace(0, 255, all_keypoints.shape[0]+2).astype(int).tolist()[1:-1])\n",
    "    boxes = sorted(all_boxes.tolist())\n",
    "    color_order = 0\n",
    "    for i, person_id in enumerate(all_keypoints[:,:,0].mean(axis=1).argsort()):\n",
    "        keypoints_transformed = affine_transform(all_keypoints[0], all_keypoints[person_id], confs[0], confs[person_id])\n",
    "        cos_sim = round(cosine_similarity(keypoints_transformed, all_keypoints[0]), 4)\n",
    "        W = all_scores[0] * all_scores[person_id]\n",
    "        W[W < 0] = 0\n",
    "        W = np.sqrt(W)\n",
    "        wght = round(1-weighted_distance(keypoints_transformed, all_keypoints[0], W), 4)\n",
    "        fontscale = min(img.shape[:2]) * 1.3e-3\n",
    "        thickness = int((min(img.shape[:2]) * 5e-3))\n",
    "        keypoints = all_keypoints[person_id, ...]\n",
    "        scores = all_scores[person_id, ...]\n",
    "        color = tuple(np.asarray(cmap(color_id[color_order])[:-1])*255)\n",
    "        x_pos = int(boxes[person_id][0])\n",
    "        y_pos = int(boxes[person_id][1])\n",
    "        if person_id == 0:\n",
    "            text = f\"ETALON\"\n",
    "        else:\n",
    "            text = f\"DANC-{person_id}\"\n",
    "        xp = int((int(boxes[person_id][2]-int(boxes[person_id][0])))/3)\n",
    "        cv2.putText(\n",
    "            img_copy, text, (x_pos+xp, y_pos-20),\n",
    "            fontFace=cv2.FONT_ITALIC, \n",
    "            fontScale=1.15, \n",
    "            color=(255, 255, 0), thickness=thickness)\n",
    "        color_order += 1\n",
    "        for kp in range(len(scores)):\n",
    "            if scores[kp] > keypoint_threshold:\n",
    "                keypoint = tuple(map(int, keypoints[kp, :2])) \n",
    "                cv2.circle(img_copy, keypoint, rd, color, -1)\n",
    "        \n",
    "        for limb_id in range(len(limbs)):\n",
    "          limb_loc1 = tuple(map(int, keypoints[limbs[limb_id][0], :2]))\n",
    "          limb_loc2 = tuple(map(int, keypoints[limbs[limb_id][1], :2]))\n",
    "          limb_score = min(all_scores[person_id, limbs[limb_id][0]], all_scores[person_id, limbs[limb_id][1]])\n",
    "          if limb_score> keypoint_threshold:\n",
    "            cv2.line(img_copy, limb_loc1, limb_loc2, color, thickness=th)\n",
    "\n",
    "    return img_copy[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file: ../video/3-danc.mp4\n",
      " width: 1920\n",
      " height: 1080\n",
      " fps: 29.97002997002997\n",
      " frame count: 90.0\n"
     ]
    }
   ],
   "source": [
    "video_file = \"../video/3-danc.mp4\"\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "frameRate = cap.get(5)\n",
    "girl = []\n",
    "while(cap.isOpened()):\n",
    "  frameId = cap.get(1) \n",
    "  ret, frame = cap.read()\n",
    "  if (ret != True):\n",
    "      break\n",
    "  else:\n",
    "    girl.append(frame)\n",
    "\n",
    "# Data about the video file being processed.  \n",
    "video_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "video_fn = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "print(f\" file: {video_file}\\n width: {video_w}\\n height: {video_h}\\n \\\n",
    "fps: {video_fps}\\n frame count: {video_fn}\")\n",
    "cap.release()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.keypointrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "keypoints = ['nose', 'left_eye','right_eye',\n",
    "             'left_ear', 'right_ear', 'left_shoulder',\n",
    "             'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "             'left_wrist', 'right_wrist', 'left_hip',\n",
    "             'right_hip', 'left_knee', 'right_knee',\n",
    "             'left_ankle', 'right_ankle']\n",
    "\n",
    "def get_limbs_from_keypoints(keypoints):\n",
    "    limbs = [\n",
    "        [keypoints.index(\"right_eye\"), \n",
    "        keypoints.index(\"nose\")],\n",
    "        [keypoints.index(\"right_eye\"), \n",
    "        keypoints.index(\"right_ear\")],\n",
    "        [keypoints.index(\"left_eye\"), \n",
    "        keypoints.index(\"nose\")],\n",
    "        [keypoints.index(\"left_eye\"), \n",
    "        keypoints.index(\"left_ear\")],\n",
    "        [keypoints.index(\"right_shoulder\"), \n",
    "        keypoints.index(\"right_elbow\")],\n",
    "        [keypoints.index(\"right_elbow\"), \n",
    "        keypoints.index(\"right_wrist\")],\n",
    "        [keypoints.index(\"left_shoulder\"), \n",
    "        keypoints.index(\"left_elbow\")],\n",
    "        [keypoints.index(\"left_elbow\"), \n",
    "        keypoints.index(\"left_wrist\")],\n",
    "        [keypoints.index(\"right_hip\"), \n",
    "        keypoints.index(\"right_knee\")],\n",
    "        [keypoints.index(\"right_knee\"), \n",
    "        keypoints.index(\"right_ankle\")],\n",
    "        [keypoints.index(\"left_hip\"), \n",
    "        keypoints.index(\"left_knee\")],\n",
    "        [keypoints.index(\"left_knee\"), \n",
    "        keypoints.index(\"left_ankle\")],\n",
    "        [keypoints.index(\"right_shoulder\"), \n",
    "        keypoints.index(\"left_shoulder\")],\n",
    "        [keypoints.index(\"right_hip\"), \n",
    "        keypoints.index(\"left_hip\")],\n",
    "        [keypoints.index(\"right_shoulder\"), \n",
    "        keypoints.index(\"right_hip\")],\n",
    "        [keypoints.index(\"left_shoulder\"), \n",
    "        keypoints.index(\"left_hip\")]]\n",
    "    return limbs\n",
    "\n",
    "limbs = get_limbs_from_keypoints(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Preparing the model for the inference.  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a skeleton and creating a list of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:32<00:00,  2.80it/s]\n"
     ]
    }
   ],
   "source": [
    "grl_frame=[]\n",
    "count = 0\n",
    "for img in tqdm(girl):\n",
    "# Transformation - obtaining an image tensor. \n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    img_tensor = transform(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        out_img = model([img_tensor])[0]\n",
    "\n",
    "    # Mask of the threshold number of objects in the frame.  \n",
    "    mask_obj = out_img['scores'] > 0.9\n",
    "\n",
    "    # Only objects that have passed through the threshold.  \n",
    "    boxes = out_img['boxes']\n",
    "    all_keys = out_img['keypoints']\n",
    "    all_scrs = out_img['keypoints_scores']\n",
    "    confidence = out_img['scores']\n",
    "    frame_keys = all_keys[mask_obj][:,:,:2].cpu().numpy()\n",
    "    frame_scores = all_scrs[mask_obj].cpu().numpy()\n",
    "    frame_confs = confidence[mask_obj].cpu().numpy()\n",
    "    frame_boxes = boxes[mask_obj].cpu().numpy()\n",
    "\n",
    "    skeletal_mdl = draw_skeleton_per_person(\n",
    "        img[:,:,::-1], \n",
    "        frame_keys,\n",
    "        frame_scores,\n",
    "        frame_confs, frame_boxes,\n",
    "        rd=6, th=3) \n",
    "\n",
    "    grl_frame.append(skeletal_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a list of frames to a video file and saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_out = cv2.VideoWriter(\n",
    "    \"out_video.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 29.97002997002997, (1920, 1080))\n",
    "for frame in grl_frame:\n",
    "    video_out.write(frame)\n",
    "video_out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
